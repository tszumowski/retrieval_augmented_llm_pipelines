# %%
# load vectors-openai-embedding-index.jsonl
import sys

sys.path.append("../../cloud_functions/url-scraper")
from util_scrape import clean_text

sys.path.append("../../cloud_functions/embedding-indexer")
from tokenization import tiktoken_len, split_by_tokenization

import jsonlines
from tqdm import tqdm


# %%
# User Config

vectors_file = "./vectors-openai-embedding-index.jsonl"
cost_per_token = 0.0004 / 1000  # 0.0004 cents per 1000 tokens

# %%
# read in the jsonlines file into data
data = list()
print(f"Reading in data from file: {vectors_file} ...")
with jsonlines.open(vectors_file) as reader:
    for obj in reader:
        data.append(obj)
print(f"Read in {len(data)} records from file.")

# %%
# Loop through each and clean the text
print("Cleaning text...")
for d in tqdm(data):
    d["metadata"]["text"] = clean_text(d["metadata"]["text"])

# %%
cleaned_file = vectors_file.replace(".jsonlines", "-cleaned.jsonl")
print(f"Writing cleaned data to file: {cleaned_file} ...")
with jsonlines.open(cleaned_file, "w") as writer:
    for d in data:
        writer.write(d)


# %%
# Tokenize the cleaned data and print the total number of tokens
print("Tokenizing cleaned data...")
n_tokens = 0
orig_size = 0
for d in tqdm(data):
    cur_text = d["metadata"]["text"]
    cur_token_len = tiktoken_len(cur_text)
    n_tokens += cur_token_len

# %%
print(f"Total number of tokens in cleaned document store: {n_tokens}")
print(f"Total cost to embed document store: ${n_tokens * cost_per_token:.2f}")

# %%
# Get all unique metadata keys
print("Getting all unique metadata keys...")
metadata_keys = set()
for d in tqdm(data):
    for k in d["metadata"].keys():
        metadata_keys.add(k)

print(f"Found {len(metadata_keys)} unique metadata keys.")
print(f"Metadata keys: {metadata_keys}")
# %%
d = random.choice(data)
kk = list(d["metadata"].keys()).copy()
# delete text from kk
kk.remove("text")
print([(k, d["metadata"][k]) for k in kk])
print(d["metadata"]["text"][0:500])

# Calculate tokenization count using the
from langchain.text_splitter import RecursiveCharacterTextSplitter

dd = d["metadata"]["text"]
tik_len = tiktoken_len(dd)
print(f"Number of tokens in text: {tik_len}")

chunk_size = 400  # number of tokens per chunk
chunk_overlap = 0  # number of tokens overlap between chunks
# %%
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=chunk_size,
    chunk_overlap=chunk_overlap,
    length_function=tiktoken_len,
    separators=["\n\n", "\n", " ", ""],
)
chunks = text_splitter.split_text(dd)
print(f"Number of chunks: {len(chunks)}")

# %%
